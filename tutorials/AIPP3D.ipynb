{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive-IPP with Non-Point FoV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# Import updated sgptools modules\n",
    "from sgptools.utils.metrics import get_reconstruction, get_rmse\n",
    "from sgptools.utils.data import Dataset\n",
    "from sgptools.core.osgpr import init_osgpr\n",
    "from sgptools.utils.tsp import run_tsp\n",
    "from sgptools.utils.gpflow import get_model_params, optimize_model\n",
    "from sgptools.methods import ContinuousSGP\n",
    "from sgptools.core.transformations import IPPTransform, SquareHeightTransform\n",
    "from gpflow.utilities import print_summary\n",
    "from sgptools.utils.misc import get_inducing_pts\n",
    "import gpflow\n",
    "\n",
    "# Plotting libraries\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import mpl_toolkits.mplot3d.art3d as art3d\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vertices(Xu, X_fov):\n",
    "    \"\"\"\n",
    "    Utility to map the inducing points to FoV vertices.\n",
    "    \"\"\"\n",
    "    X_fov = X_fov.reshape(len(Xu), -1, 2)\n",
    "    vertices = np.zeros((len(Xu), 5, 2))\n",
    "    vertices[:, 0] = X_fov.min(axis=1)\n",
    "    vertices[:, 1] = np.array([X_fov[:, :, 0].min(axis=1),\n",
    "                              X_fov[:, :, 1].max(axis=1)]).T\n",
    "    vertices[:, 2] = X_fov.max(axis=1)\n",
    "    vertices[:, 3] = np.array([X_fov[:, :, 0].max(axis=1),\n",
    "                              X_fov[:, :, 1].min(axis=1)]).T\n",
    "    vertices[:, 4] = X_fov.mean(axis=1)\n",
    "\n",
    "    dists = pairwise_distances(vertices[:, 4], Y=Xu[:, :2],\n",
    "                               metric='euclidean')\n",
    "    _, idx = linear_sum_assignment(dists)\n",
    "\n",
    "    vertices[:, 4] = Xu[idx][:, :2]\n",
    "    return vertices\n",
    "\n",
    "def plot_results(X_inducing, X_fov, current_idx=0, fname=None):\n",
    "    \"\"\"\n",
    "    Method to plot the solution with FoVs and environment reconstruction.\n",
    "    \"\"\"\n",
    "    # Setup 3D plot\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(111, projection='3d', computed_zorder=False)\n",
    "\n",
    "    current_idx += 1 # Increment by one for plotting\n",
    "\n",
    "    # Plot the reconstructed environment state\n",
    "    num_fov_pts = len(X_fov)//len(X_inducing)\n",
    "    sensor_data = dataset.get_sensor_data(X_fov[:num_fov_pts*(current_idx)], \n",
    "                                          max_samples=1500)\n",
    "    y_pred, _ = get_reconstruction(sensor_data, dataset.get_test()[0],\n",
    "                                   noise_variance_opt, kernel_opt)\n",
    "    y_train_min, y_train_max = dataset.get_train()[1].min(), dataset.get_train()[1].max()\n",
    "    norm = colors.Normalize(y_train_min, y_train_max)\n",
    "    test_dim_shape = int(np.sqrt(dataset.get_test()[0].shape[0]))\n",
    "    _ = ax.plot_surface(dataset.get_test()[0][:, 0].reshape(test_dim_shape, test_dim_shape),\n",
    "                        dataset.get_test()[0][:, 1].reshape(test_dim_shape, test_dim_shape),\n",
    "                        np.atleast_2d(-0.1),\n",
    "                        facecolors=plt.cm.jet(norm(y_pred.reshape(test_dim_shape, test_dim_shape))),\n",
    "                        shade=False,\n",
    "                        alpha=0.8,\n",
    "                        zorder=0)\n",
    "\n",
    "    # Plot the solution path\n",
    "    ax.scatter(X_inducing[:current_idx, 0],\n",
    "               X_inducing[:current_idx, 1],\n",
    "               X_inducing[:current_idx, 2], c='C3')\n",
    "    ax.scatter(X_inducing[current_idx:, 0],\n",
    "               X_inducing[current_idx:, 1],\n",
    "               X_inducing[current_idx:, 2], c='C2')\n",
    "    ax.plot(X_inducing[:, 0], X_inducing[:, 1], X_inducing[:, 2], 'k-')\n",
    "\n",
    "    # Plot the FoV vertices\n",
    "    vertices = get_vertices(X_inducing, X_fov)\n",
    "    for i in range(vertices.shape[0]):\n",
    "        color = 'C3' if i < current_idx else 'C2'\n",
    "        verts = []\n",
    "        verts.append([vertices[i, 0], vertices[i, 1],\n",
    "                      vertices[i, 2], vertices[i, 3]])\n",
    "        fov = Polygon(np.array(verts)[0, :, :2],\n",
    "                      linewidth=1.5,\n",
    "                      edgecolor=color,\n",
    "                      facecolor=color,\n",
    "                      fill=False,\n",
    "                      zorder=15)\n",
    "        ax.add_patch(fov)\n",
    "        art3d.pathpatch_2d_to_3d(fov)\n",
    "\n",
    "    # Configure other plot settings\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    ax.set_xlim(np.min(dataset.get_train()[0][:, 0])-0.5, np.max(dataset.get_train()[0][:, 0])+0.5)\n",
    "    ax.set_ylim(np.min(dataset.get_train()[0][:, 1])-0.5, np.max(dataset.get_train()[0][:, 1])+0.5)\n",
    "    ax.set_zlim(0, 14)\n",
    "\n",
    "    ax.view_init(elev=30, azim=45+180)\n",
    "    ax.set_title(f'Waypoint: {current_idx}, RMSE: {get_rmse(y_pred, dataset.get_test()[1]):.2f}\\nData Field Reconstruction', y=0.99)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aipp(X_train, ipp_model, Xu_init, dataset):\n",
    "    \"\"\"\n",
    "    Method to run AIPP using the SGP-based approach.\n",
    "    \"\"\"\n",
    "    total_time_param = 0\n",
    "    total_time_ipp = 0\n",
    "    num_robots = Xu_init.shape[0]\n",
    "    num_waypoints = Xu_init.shape[1]\n",
    "    curr_sol = Xu_init.copy()\n",
    "    \n",
    "    # Get the initial expanded path for plotting\n",
    "    expanded_path = ipp_model.transform.expand(curr_sol.reshape(-1, 3), \n",
    "                                               expand_sensor_model=False)\n",
    "    if not isinstance(expanded_path, np.ndarray):\n",
    "        expanded_path = expanded_path.numpy()\n",
    "\n",
    "    plot_results(expanded_path,\n",
    "                 ipp_model.transform.expand(curr_sol.reshape(-1, 3)).numpy())\n",
    "\n",
    "    # Initialize the hyperparameters and SSGP\n",
    "    init_kernel, init_noise_variance = ipp_model.get_hyperparameters()\n",
    "    param_model = init_osgpr(X_train,\n",
    "                             num_inducing=40,\n",
    "                             kernel=init_kernel,\n",
    "                             noise_variance=init_noise_variance)\n",
    "\n",
    "    sol_data_X = []\n",
    "    sol_data_y = []\n",
    "    for time_step in range(num_waypoints):\n",
    "        # Get a new batch of data from the last visited waypoint\n",
    "        last_visited = curr_sol[:, time_step].copy()\n",
    "        \n",
    "        # Apply the sensor model expansion with fixed points IPP expansion\n",
    "        data_pts = ipp_model.transform.sensor_model.expand(tf.constant(last_visited)).numpy()\n",
    "        data_X_batch, data_y_batch = dataset.get_sensor_data(data_pts)\n",
    "        sol_data_X.extend(data_X_batch)\n",
    "        sol_data_y.extend(data_y_batch)\n",
    "\n",
    "        # Skip param and path update for the last waypoint\n",
    "        if time_step == num_waypoints - 1:\n",
    "            break\n",
    "\n",
    "        # Update the SSGP hyperparameter model\n",
    "        param_model.update((np.array(data_X_batch),\n",
    "                            np.array(data_y_batch)),\n",
    "                           update_inducing=True)\n",
    "        # Train only the kernel parameters and noise variance.\n",
    "        start_time = time()\n",
    "        optimize_model(param_model,\n",
    "                       trainable_variables=param_model.trainable_variables)\n",
    "        end_time = time()\n",
    "        total_time_param += end_time - start_time\n",
    "        print_summary(param_model.kernel)\n",
    "        print_summary(param_model.likelihood)\n",
    "\n",
    "        # Update the SGP-IPP model\n",
    "        Xu_visited = curr_sol.copy()[:, :time_step + 1]\n",
    "        ipp_model.transform.update_Xu_fixed(Xu_visited)\n",
    "        ipp_model.update(param_model.kernel, \n",
    "                         param_model.likelihood.variance.numpy())\n",
    "        start_time = time()\n",
    "        \n",
    "        # Optimize the IPP model\n",
    "        curr_sol = ipp_model.optimize(verbose=False)\n",
    "        end_time = time()\n",
    "        total_time_ipp += end_time - start_time\n",
    "        \n",
    "        # Get the expanded path for plotting\n",
    "        expanded_path = ipp_model.transform.expand(tf.constant(curr_sol.reshape(-1, 3)), \n",
    "                                                   expand_sensor_model=False)\n",
    "        if not isinstance(expanded_path, np.ndarray):\n",
    "            expanded_path = expanded_path.numpy()\n",
    "\n",
    "        plot_results(expanded_path,\n",
    "                     ipp_model.transform.expand(tf.constant(curr_sol.reshape(-1, 3))).numpy(),\n",
    "                     time_step + 1)\n",
    "\n",
    "    return np.array(sol_data_X), np.array(sol_data_y), total_time_param, total_time_ipp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data and initial SGP hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the synthetic elevation data using the new Dataset class\n",
    "dataset = Dataset(random_seed=0,\n",
    "                  shape=(75, 75),\n",
    "                  num_test=75*75)\n",
    "X_train, y_train = dataset.get_train()\n",
    "X_test, y_test = dataset.get_test()\n",
    "\n",
    "# Train a GP and get the model parameters\n",
    "print('Optimized Hyperparameters')\n",
    "_, noise_variance_opt, kernel_opt = get_model_params(X_train, y_train,\n",
    "                                                     lengthscales=[1.0, 1.0],\n",
    "                                                     optimizer='scipy.L-BFGS-B')\n",
    "\n",
    "# Plot the data field\n",
    "test_dim_shape = int(np.sqrt(X_test.shape[0]))\n",
    "plt.imshow(y_test.reshape(test_dim_shape, test_dim_shape), cmap=\"jet\", origin='lower')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Elevation Data Field')\n",
    "plt.colorbar(label='Normalized Elevation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the AIPP solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_robots = 1\n",
    "num_placements = 12\n",
    "pts_per_side = 7  # Use a pts_per_side x pts_per_side point FoV approximation\n",
    "\n",
    "# Setup the initial AIPP hyperparameters\n",
    "print('Initial AIPP Hyperparameters')\n",
    "_, noise_variance, kernel = get_model_params(X_train, y_train,\n",
    "                                             lengthscales=[1.0, 1.0],\n",
    "                                             max_steps=0)\n",
    "\n",
    "# Get initial inducing points\n",
    "Xu_init = get_inducing_pts(X_train, num_placements)\n",
    "# Initialize the height dimension at 2.0 for all points\n",
    "Xu_init = np.concatenate((Xu_init, np.ones((Xu_init.shape[0], 1)) * 2.0), axis=1)\n",
    "paths, _ = run_tsp(Xu_init[:, :2])\n",
    "if paths:\n",
    "    Xu_init[:, :2] = paths[0]\n",
    "\n",
    "# Initialize the non-point FoV transform\n",
    "fov_transform = SquareHeightTransform(pts_per_side=pts_per_side)\n",
    "transform = IPPTransform(num_dim=3,\n",
    "                         num_robots=1,\n",
    "                         sensor_model=fov_transform,\n",
    "                         aggregate_fov=True)\n",
    "\n",
    "# Initialize the IPP model using the new ContinuousSGP method\n",
    "ipp_model = ContinuousSGP(\n",
    "    num_sensing=num_placements,\n",
    "    X_objective=X_train,\n",
    "    noise_variance=noise_variance,\n",
    "    kernel=kernel,\n",
    "    transform=transform,\n",
    "    X_init=Xu_init,\n",
    ")\n",
    "\n",
    "# Generate the SGP-based AIPP solution\n",
    "_, _, param_time, ipp_time = run_aipp(X_train,\n",
    "                                      ipp_model,\n",
    "                                      Xu_init.reshape(1, -1, 3),\n",
    "                                      dataset)\n",
    "\n",
    "print(f'Total Hyperparameter Update Time:   {param_time:.2f}s')\n",
    "print(f'Total IPP Update Time:              {ipp_time:.2f}s')\n",
    "\n",
    "print(f'Average Hyperparameter Update Time: {param_time/num_placements:.2f}s')\n",
    "print(f'Average IPP Update Time:            {ipp_time/num_placements:.2f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
